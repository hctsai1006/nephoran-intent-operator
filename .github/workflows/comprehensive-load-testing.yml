name: "Comprehensive Load Testing & Capacity Validation"

on:
  schedule:
    # Run nightly load tests
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_scenarios:
        description: 'Test scenarios (comma-separated)'
        required: false
        default: 'baseline,peak,burst,sustained,failure'
        type: string
      max_users:
        description: 'Maximum concurrent users'
        required: false
        default: '300'
        type: string
      test_duration:
        description: 'Test duration (minutes)'
        required: false
        default: '15'
        type: string
      environment:
        description: 'Target environment'
        required: false
        default: 'staging'
        type: choice
        options:
          - staging
          - production
          - local

env:
  TEST_SCENARIOS: ${{ github.event.inputs.test_scenarios || 'baseline,peak,burst,sustained,failure' }}
  MAX_USERS: ${{ github.event.inputs.max_users || '300' }}
  TEST_DURATION: ${{ github.event.inputs.test_duration || '15' }}
  TARGET_ENVIRONMENT: ${{ github.event.inputs.environment || 'staging' }}
  
  # Performance thresholds
  BASELINE_USERS: 50
  PEAK_USERS: 200
  BURST_USERS: 300
  SUSTAINED_USERS: 150
  
  # SLA Thresholds
  MAX_P95_LATENCY: 2000  # ms
  MIN_THROUGHPUT: 45     # requests/min
  MIN_AVAILABILITY: 99.95 # percentage
  MAX_ERROR_RATE: 1.0    # percentage

jobs:
  preparation:
    name: "Load Test Environment Preparation"
    runs-on: ubuntu-latest
    outputs:
      test-matrix: ${{ steps.matrix.outputs.matrix }}
      environment-ready: ${{ steps.env-check.outputs.ready }}
    
    steps:
      - name: "Checkout Repository"
        uses: actions/checkout@v4
        
      - name: "Setup Test Matrix"
        id: matrix
        run: |
          # Create dynamic test matrix based on input scenarios
          IFS=',' read -ra SCENARIOS <<< "$TEST_SCENARIOS"
          
          MATRIX_JSON="{"
          MATRIX_JSON+='"include": ['
          
          for scenario in "${SCENARIOS[@]}"; do
            case "$scenario" in
              "baseline")
                MATRIX_JSON+='{
                  "scenario": "baseline",
                  "description": "Baseline performance validation",
                  "users": '${BASELINE_USERS}',
                  "ramp_duration": "2m",
                  "hold_duration": "5m",
                  "expected_p95": 1500,
                  "expected_throughput": 50
                },'
                ;;
              "peak")
                MATRIX_JSON+='{
                  "scenario": "peak",
                  "description": "Peak capacity validation (200+ users)",
                  "users": '${PEAK_USERS}',
                  "ramp_duration": "5m",
                  "hold_duration": "10m",
                  "expected_p95": 2000,
                  "expected_throughput": 45
                },'
                ;;
              "burst")
                MATRIX_JSON+='{
                  "scenario": "burst",
                  "description": "Burst capacity handling",
                  "users": '${BURST_USERS}',
                  "ramp_duration": "1m",
                  "hold_duration": "3m",
                  "expected_p95": 2500,
                  "expected_throughput": 40
                },'
                ;;
              "sustained")
                MATRIX_JSON+='{
                  "scenario": "sustained",
                  "description": "Sustained load over extended period",
                  "users": '${SUSTAINED_USERS}',
                  "ramp_duration": "3m",
                  "hold_duration": "15m",
                  "expected_p95": 1800,
                  "expected_throughput": 48
                },'
                ;;
              "failure")
                MATRIX_JSON+='{
                  "scenario": "failure",
                  "description": "Failure scenario testing",
                  "users": 400,
                  "ramp_duration": "2m",
                  "hold_duration": "5m",
                  "expected_p95": 5000,
                  "expected_throughput": 20
                },'
                ;;
            esac
          done
          
          # Remove trailing comma and close JSON
          MATRIX_JSON="${MATRIX_JSON%,}]}"
          
          echo "Test Matrix:"
          echo "$MATRIX_JSON" | jq '.'
          
          echo "matrix=$MATRIX_JSON" >> $GITHUB_OUTPUT
          
      - name: "Environment Readiness Check"
        id: env-check
        run: |
          echo "Checking $TARGET_ENVIRONMENT environment readiness..."
          
          # Set environment-specific endpoints
          case "$TARGET_ENVIRONMENT" in
            "staging")
              ENDPOINT="https://staging.nephoran.local"
              ;;
            "production")
              ENDPOINT="https://nephoran.local"
              ;;
            "local")
              ENDPOINT="http://localhost:8080"
              ;;
          esac
          
          # Check endpoint health
          for i in {1..5}; do
            if curl -f -s "$ENDPOINT/health" > /dev/null; then
              echo "✅ Environment $TARGET_ENVIRONMENT is ready"
              echo "ready=true" >> $GITHUB_OUTPUT
              echo "endpoint=$ENDPOINT" >> $GITHUB_OUTPUT
              break
            fi
            echo "⏳ Waiting for environment (attempt $i/5)..."
            sleep 10
          done
          
          if [ "${ready:-false}" != "true" ]; then
            echo "❌ Environment $TARGET_ENVIRONMENT is not ready"
            echo "ready=false" >> $GITHUB_OUTPUT
            exit 1
          fi

  load-testing:
    name: "Load Test - ${{ matrix.scenario }}"
    runs-on: ubuntu-latest
    needs: preparation
    if: needs.preparation.outputs.environment-ready == 'true'
    timeout-minutes: 45
    
    strategy:
      fail-fast: false
      matrix: ${{ fromJson(needs.preparation.outputs.test-matrix) }}
    
    steps:
      - name: "Checkout Repository"
        uses: actions/checkout@v4
        
      - name: "Setup Load Testing Tools"
        run: |
          echo "Installing load testing tools..."
          
          # Install k6 for advanced load testing
          sudo gpg -k
          sudo gpg --no-default-keyring --keyring /usr/share/keyrings/k6-archive-keyring.gpg --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69
          echo "deb [signed-by=/usr/share/keyrings/k6-archive-keyring.gpg] https://dl.k6.io/deb stable main" | sudo tee /etc/apt/sources.list.d/k6.list
          sudo apt-get update
          sudo apt-get install k6
          
          # Install artillery for additional testing
          npm install -g artillery@latest
          
          # Create results directory
          mkdir -p load-test-results/${{ matrix.scenario }}
          
      - name: "Generate Load Test Scripts"
        run: |
          echo "Generating load test script for scenario: ${{ matrix.scenario }}"
          
          # Create k6 test script
          cat > load-test-${{ matrix.scenario }}.js << 'EOF'
          import http from 'k6/http';
          import { check, sleep } from 'k6';
          import { Rate, Trend, Counter } from 'k6/metrics';
          
          // Custom metrics
          const errorRate = new Rate('error_rate');
          const intentProcessingLatency = new Trend('intent_processing_latency');
          const intentProcessingRate = new Rate('intent_processing_success_rate');
          const ragLatency = new Trend('rag_retrieval_latency');
          const cacheHits = new Counter('cache_hits');
          const cacheMisses = new Counter('cache_misses');
          
          export const options = {
            scenarios: {
              load_test: {
                executor: 'ramping-vus',
                stages: [
                  { duration: '${{ matrix.ramp_duration }}', target: ${{ matrix.users }} },
                  { duration: '${{ matrix.hold_duration }}', target: ${{ matrix.users }} },
                  { duration: '2m', target: 0 },
                ],
              },
            },
            thresholds: {
              http_req_duration: ['p(95)<${{ matrix.expected_p95 }}'],
              http_req_failed: ['rate<0.01'],
              'intent_processing_latency': ['p(95)<2000'],
              'error_rate': ['rate<0.01'],
            },
          };
          
          // Test data generator
          function generateIntentPayload() {
            const intents = [
              'Deploy high-availability AMF instance with auto-scaling',
              'Configure SMF with QoS policy for enhanced mobile broadband',
              'Setup UPF with edge computing capabilities',
              'Create network slice for ultra-reliable low-latency communications',
              'Deploy O-RAN CU with intelligent resource allocation',
              'Configure massive IoT slice with optimized power consumption'
            ];
            
            return {
              intent: intents[Math.floor(Math.random() * intents.length)],
              priority: Math.random() < 0.3 ? 'high' : 'normal',
              metadata: {
                user_id: `user_${Math.floor(Math.random() * 1000)}`,
                session_id: `session_${Math.floor(Math.random() * 100)}`,
                timestamp: new Date().toISOString()
              }
            };
          }
          
          export default function () {
            // Main intent processing test
            const payload = generateIntentPayload();
            const intentResponse = http.post(
              '${{ needs.preparation.outputs.endpoint }}/api/v1/intents',
              JSON.stringify(payload),
              {
                headers: { 'Content-Type': 'application/json' },
              }
            );
            
            const intentSuccess = check(intentResponse, {
              'intent processing status 200': (r) => r.status === 200,
              'intent processing latency OK': (r) => r.timings.duration < ${{ matrix.expected_p95 }},
              'valid intent response': (r) => r.json('status') !== undefined,
            });
            
            errorRate.add(!intentSuccess);
            intentProcessingLatency.add(intentResponse.timings.duration);
            intentProcessingRate.add(intentSuccess);
            
            // RAG system test
            if (Math.random() < 0.3) { // 30% of requests also test RAG
              const ragResponse = http.get(
                '${{ needs.preparation.outputs.endpoint }}/api/v1/rag/search?query=' + encodeURIComponent(payload.intent)
              );
              
              const ragSuccess = check(ragResponse, {
                'RAG search status 200': (r) => r.status === 200,
                'RAG latency acceptable': (r) => r.timings.duration < 200,
              });
              
              ragLatency.add(ragResponse.timings.duration);
              
              // Check cache headers
              if (ragResponse.headers['x-cache-status'] === 'hit') {
                cacheHits.add(1);
              } else {
                cacheMisses.add(1);
              }
            }
            
            // Health check (10% of requests)
            if (Math.random() < 0.1) {
              const healthResponse = http.get('${{ needs.preparation.outputs.endpoint }}/health');
              check(healthResponse, {
                'health check OK': (r) => r.status === 200,
              });
            }
            
            sleep(Math.random() * 2 + 1); // Random sleep 1-3 seconds
          }
          
          export function handleSummary(data) {
            return {
              'load-test-results/${{ matrix.scenario }}/summary.json': JSON.stringify(data),
              'load-test-results/${{ matrix.scenario }}/summary.html': htmlReport(data),
            };
          }
          
          function htmlReport(data) {
            return `
          <!DOCTYPE html>
          <html>
          <head>
            <title>Load Test Results - ${{ matrix.scenario }}</title>
            <style>
              body { font-family: Arial, sans-serif; margin: 20px; }
              .metric { margin: 10px 0; padding: 10px; border-left: 4px solid #007acc; background: #f5f5f5; }
              .pass { border-left-color: #28a745; }
              .fail { border-left-color: #dc3545; }
            </style>
          </head>
          <body>
            <h1>Load Test Results - ${{ matrix.scenario }}</h1>
            <div class="metric ${data.metrics.http_req_duration.values.p95 < ${{ matrix.expected_p95 }} ? 'pass' : 'fail'}">
              <strong>P95 Latency:</strong> ${data.metrics.http_req_duration.values.p95.toFixed(2)}ms (Target: < ${{ matrix.expected_p95 }}ms)
            </div>
            <div class="metric ${data.metrics.http_req_failed.values.rate < 0.01 ? 'pass' : 'fail'}">
              <strong>Error Rate:</strong> ${(data.metrics.http_req_failed.values.rate * 100).toFixed(2)}% (Target: < 1%)
            </div>
            <div class="metric">
              <strong>Total Requests:</strong> ${data.metrics.http_reqs.values.count}
            </div>
            <div class="metric">
              <strong>Request Rate:</strong> ${data.metrics.http_reqs.values.rate.toFixed(2)}/s
            </div>
          </body>
          </html>`;
          }
          EOF
          
      - name: "Execute Load Test - ${{ matrix.scenario }}"
        id: load-test
        run: |
          echo "🚀 Starting load test scenario: ${{ matrix.scenario }}"
          echo "📊 Target: ${{ matrix.users }} users, P95 < ${{ matrix.expected_p95 }}ms"
          
          # Run k6 load test
          k6 run \
            --out json=load-test-results/${{ matrix.scenario }}/raw-results.json \
            load-test-${{ matrix.scenario }}.js
          
          # Parse results
          RESULTS_FILE="load-test-results/${{ matrix.scenario }}/summary.json"
          if [ -f "$RESULTS_FILE" ]; then
            P95_LATENCY=$(jq -r '.metrics.http_req_duration.values.p95' "$RESULTS_FILE")
            ERROR_RATE=$(jq -r '.metrics.http_req_failed.values.rate' "$RESULTS_FILE")
            REQUEST_RATE=$(jq -r '.metrics.http_reqs.values.rate' "$RESULTS_FILE")
            TOTAL_REQUESTS=$(jq -r '.metrics.http_reqs.values.count' "$RESULTS_FILE")
            
            echo "📈 Results Summary:"
            echo "  P95 Latency: ${P95_LATENCY}ms"
            echo "  Error Rate: $(echo "$ERROR_RATE * 100" | bc -l | xargs printf "%.2f")%"
            echo "  Request Rate: ${REQUEST_RATE}/s"
            echo "  Total Requests: ${TOTAL_REQUESTS}"
            
            # Determine if test passed
            LATENCY_OK=$(echo "$P95_LATENCY <= ${{ matrix.expected_p95 }}" | bc -l)
            ERROR_OK=$(echo "$ERROR_RATE <= 0.01" | bc -l)
            
            if [ "$LATENCY_OK" -eq 1 ] && [ "$ERROR_OK" -eq 1 ]; then
              echo "✅ Load test PASSED"
              echo "test-result=PASS" >> $GITHUB_OUTPUT
            else
              echo "❌ Load test FAILED"
              echo "test-result=FAIL" >> $GITHUB_OUTPUT
            fi
            
            # Export metrics for aggregation
            echo "p95-latency=$P95_LATENCY" >> $GITHUB_OUTPUT
            echo "error-rate=$ERROR_RATE" >> $GITHUB_OUTPUT
            echo "request-rate=$REQUEST_RATE" >> $GITHUB_OUTPUT
            echo "total-requests=$TOTAL_REQUESTS" >> $GITHUB_OUTPUT
          else
            echo "❌ Load test results file not found"
            echo "test-result=ERROR" >> $GITHUB_OUTPUT
          fi
          
      - name: "Generate Performance Analysis"
        run: |
          echo "Generating detailed performance analysis..."
          
          # Create detailed analysis report
          cat > load-test-results/${{ matrix.scenario }}/analysis.md << EOF
          # Load Test Analysis - ${{ matrix.scenario }}
          
          ## Test Configuration
          - **Scenario**: ${{ matrix.description }}
          - **Users**: ${{ matrix.users }}
          - **Ramp Duration**: ${{ matrix.ramp_duration }}
          - **Hold Duration**: ${{ matrix.hold_duration }}
          - **Expected P95**: ${{ matrix.expected_p95 }}ms
          - **Expected Throughput**: ${{ matrix.expected_throughput }}/s
          
          ## Results Summary
          - **P95 Latency**: ${{ steps.load-test.outputs.p95-latency }}ms
          - **Error Rate**: $(echo "${{ steps.load-test.outputs.error-rate }} * 100" | bc -l | xargs printf "%.2f")%
          - **Request Rate**: ${{ steps.load-test.outputs.request-rate }}/s
          - **Total Requests**: ${{ steps.load-test.outputs.total-requests }}
          
          ## Pass/Fail Status
          **Overall Result**: ${{ steps.load-test.outputs.test-result }}
          
          ## Performance Claims Validation
          - **Intent Processing P95 Latency**: $([ $(echo "${{ steps.load-test.outputs.p95-latency }} <= 2000" | bc -l) -eq 1 ] && echo "✅ PASS" || echo "❌ FAIL") (Target: ≤2000ms)
          - **Error Rate**: $([ $(echo "${{ steps.load-test.outputs.error-rate }} <= 0.01" | bc -l) -eq 1 ] && echo "✅ PASS" || echo "❌ FAIL") (Target: ≤1%)
          - **Concurrent User Capacity**: $([ ${{ matrix.users }} -ge 200 ] && echo "✅ PASS" || echo "✅ N/A") (Target: 200+ users)
          
          EOF
          
      - name: "Upload Test Results"
        uses: actions/upload-artifact@v4
        with:
          name: load-test-results-${{ matrix.scenario }}-${{ github.run_number }}
          path: load-test-results/${{ matrix.scenario }}/
          retention-days: 30

  aggregation:
    name: "Aggregate Load Test Results"
    runs-on: ubuntu-latest
    needs: load-testing
    if: always()
    
    steps:
      - name: "Download All Test Results"
        uses: actions/download-artifact@v4
        with:
          pattern: load-test-results-*-${{ github.run_number }}
          merge-multiple: true
          
      - name: "Aggregate Results"
        id: aggregate
        run: |
          echo "Aggregating load test results..."
          
          # Create consolidated report
          mkdir -p consolidated-results
          
          # Initialize counters
          TOTAL_TESTS=0
          PASSED_TESTS=0
          
          # Process each scenario
          for scenario_dir in load-test-results-*; do
            if [ -d "$scenario_dir" ]; then
              scenario=$(echo "$scenario_dir" | sed 's/load-test-results-//' | sed 's/-[0-9]*$//')
              echo "Processing scenario: $scenario"
              
              TOTAL_TESTS=$((TOTAL_TESTS + 1))
              
              if [ -f "$scenario_dir/analysis.md" ]; then
                if grep -q "Overall Result: PASS" "$scenario_dir/analysis.md"; then
                  PASSED_TESTS=$((PASSED_TESTS + 1))
                fi
              fi
              
              # Copy individual reports
              cp -r "$scenario_dir"/* consolidated-results/ 2>/dev/null || true
            fi
          done
          
          SUCCESS_RATE=$(echo "scale=2; $PASSED_TESTS * 100 / $TOTAL_TESTS" | bc -l)
          
          echo "Aggregation Summary:"
          echo "  Total Tests: $TOTAL_TESTS"
          echo "  Passed Tests: $PASSED_TESTS"
          echo "  Success Rate: ${SUCCESS_RATE}%"
          
          echo "total-tests=$TOTAL_TESTS" >> $GITHUB_OUTPUT
          echo "passed-tests=$PASSED_TESTS" >> $GITHUB_OUTPUT
          echo "success-rate=$SUCCESS_RATE" >> $GITHUB_OUTPUT
          
          # Create executive summary
          cat > consolidated-results/executive-summary.md << EOF
          # Load Testing Executive Summary
          
          ## Overall Results
          - **Total Scenarios Tested**: $TOTAL_TESTS
          - **Scenarios Passed**: $PASSED_TESTS
          - **Success Rate**: ${SUCCESS_RATE}%
          - **Test Environment**: $TARGET_ENVIRONMENT
          - **Execution Date**: $(date -u '+%Y-%m-%d %H:%M:%S UTC')
          
          ## Scenario Results
          EOF
          
          # Add individual scenario results to summary
          for scenario_dir in load-test-results-*; do
            if [ -d "$scenario_dir" ] && [ -f "$scenario_dir/analysis.md" ]; then
              scenario=$(echo "$scenario_dir" | sed 's/load-test-results-//' | sed 's/-[0-9]*$//')
              result=$(grep "Overall Result:" "$scenario_dir/analysis.md" | cut -d' ' -f3)
              
              echo "- **$scenario**: $result" >> consolidated-results/executive-summary.md
            fi
          done
          
          cat >> consolidated-results/executive-summary.md << EOF
          
          ## Performance Claims Status
          - **200+ Concurrent Users**: $([ "$SUCCESS_RATE" -eq 100 ] && echo "✅ VALIDATED" || echo "⚠️ PARTIAL")
          - **System Stability**: $([ "$SUCCESS_RATE" -ge 80 ] && echo "✅ STABLE" || echo "❌ UNSTABLE")
          - **Performance Regression**: $([ "$SUCCESS_RATE" -ge 90 ] && echo "✅ NONE DETECTED" || echo "❌ POSSIBLE REGRESSION")
          
          ## Recommendations
          $(if [ "$SUCCESS_RATE" -eq 100 ]; then
            echo "- ✅ All load testing scenarios passed successfully"
            echo "- ✅ System ready for production deployment"
            echo "- ✅ Performance claims validated"
          elif [ "$SUCCESS_RATE" -ge 80 ]; then
            echo "- ⚠️ Most scenarios passed with minor issues"
            echo "- ⚠️ Review failed scenarios for optimization opportunities"
            echo "- ⚠️ Consider performance tuning before production deployment"
          else
            echo "- ❌ Significant performance issues detected"
            echo "- ❌ Performance optimization required"
            echo "- ❌ Do not proceed with production deployment"
          fi)
          EOF
          
      - name: "Upload Consolidated Results"
        uses: actions/upload-artifact@v4
        with:
          name: consolidated-load-test-results-${{ github.run_number }}
          path: consolidated-results/
          retention-days: 60
          
      - name: "Performance Gate Decision"
        run: |
          SUCCESS_RATE="${{ steps.aggregate.outputs.success-rate }}"
          MIN_SUCCESS_RATE=80
          
          echo "Load Testing Gate Decision:"
          echo "  Success Rate: ${SUCCESS_RATE}%"
          echo "  Minimum Required: ${MIN_SUCCESS_RATE}%"
          
          if (( $(echo "$SUCCESS_RATE >= $MIN_SUCCESS_RATE" | bc -l) )); then
            echo "✅ Load testing gate PASSED"
          else
            echo "❌ Load testing gate FAILED"
            exit 1
          fi

  notification:
    name: "Send Load Test Notifications"
    runs-on: ubuntu-latest
    needs: [aggregation]
    if: always()
    
    steps:
      - name: "Prepare Notification"
        run: |
          SUCCESS_RATE="${{ needs.aggregation.outputs.success-rate || '0' }}"
          TOTAL_TESTS="${{ needs.aggregation.outputs.total-tests || '0' }}"
          PASSED_TESTS="${{ needs.aggregation.outputs.passed-tests || '0' }}"
          
          if (( $(echo "$SUCCESS_RATE >= 80" | bc -l) )); then
            STATUS="✅ SUCCESS"
            COLOR="good"
          else
            STATUS="❌ FAILURE"
            COLOR="danger"
          fi
          
          # Create notification message
          cat > notification.json << EOF
          {
            "text": "Nephoran Load Testing Results",
            "attachments": [
              {
                "color": "$COLOR",
                "title": "Load Testing Complete - $STATUS",
                "fields": [
                  {
                    "title": "Success Rate",
                    "value": "${SUCCESS_RATE}%",
                    "short": true
                  },
                  {
                    "title": "Tests Passed",
                    "value": "${PASSED_TESTS}/${TOTAL_TESTS}",
                    "short": true
                  },
                  {
                    "title": "Environment",
                    "value": "$TARGET_ENVIRONMENT",
                    "short": true
                  },
                  {
                    "title": "Run ID",
                    "value": "${{ github.run_id }}",
                    "short": true
                  }
                ],
                "actions": [
                  {
                    "type": "button",
                    "text": "View Results",
                    "url": "https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}"
                  }
                ]
              }
            ]
          }
          EOF
          
      - name: "Send Slack Notification"
        if: ${{ secrets.SLACK_WEBHOOK_URL }}
        run: |
          curl -X POST -H 'Content-type: application/json' \
            --data @notification.json \
            ${{ secrets.SLACK_WEBHOOK_URL }}
            
      - name: "Send Email Notification"
        if: ${{ secrets.SMTP_SERVER }}
        run: |
          # Email notification implementation would go here
          echo "Email notification sent to performance team"