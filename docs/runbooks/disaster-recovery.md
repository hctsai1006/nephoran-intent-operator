# Nephoran Intent Operator - Disaster Recovery Procedures

## Table of Contents
1. [Overview](#overview)
2. [RTO/RPO Targets](#rtorpo-targets)
3. [Backup Verification](#backup-verification)
4. [Failover Procedures](#failover-procedures)
5. [Data Restoration Steps](#data-restoration-steps)
6. [Service Validation](#service-validation)
7. [Business Continuity](#business-continuity)
8. [Recovery Testing](#recovery-testing)
9. [Communication Plans](#communication-plans)

## Overview

This runbook provides comprehensive disaster recovery procedures for the Nephoran Intent Operator. It covers various disaster scenarios from component failures to complete site outages, ensuring business continuity and data protection.

**Disaster Recovery Scope:**
- Complete cluster/region failure
- Data center outage
- Critical component corruption
- Security incidents requiring isolation
- Network partitioning
- Storage system failures

**Key Principles:**
- Minimize downtime and data loss
- Maintain data integrity throughout recovery
- Follow tested procedures only
- Validate each recovery step before proceeding
- Document all actions taken during recovery

## RTO/RPO Targets

### Recovery Time Objectives (RTO)

| Disaster Type | RTO Target | Maximum Acceptable |
|---------------|------------|-------------------|
| Single Component Failure | 5 minutes | 15 minutes |
| Multiple Component Failure | 15 minutes | 30 minutes |
| Database Corruption | 30 minutes | 1 hour |
| Complete Cluster Failure | 1 hour | 2 hours |
| Regional Disaster | 4 hours | 8 hours |
| Complete Site Loss | 8 hours | 24 hours |

### Recovery Point Objectives (RPO)

| Data Type | RPO Target | Maximum Acceptable |
|-----------|------------|-------------------|
| NetworkIntent State | 1 minute | 5 minutes |
| Knowledge Base Data | 15 minutes | 1 hour |
| Configuration Data | 1 hour | 4 hours |
| Audit Logs | 5 minutes | 15 minutes |
| Metrics/Monitoring Data | 1 minute | 5 minutes |

### Service Level Objectives (SLO)

```yaml
Availability Targets:
  - Production: 99.95% (26.28 minutes downtime/month)
  - Staging: 99.9% (43.8 minutes downtime/month)
  - Development: 99.5% (3.65 hours downtime/month)

Recovery Success Rate:
  - Target: 99% of recovery procedures succeed within RTO
  - Measurement: Monthly recovery test success rate

Data Integrity:
  - Target: 100% data consistency after recovery
  - Zero data loss for committed transactions
```

## Backup Verification

### Automated Backup Health Checks

```bash
# Comprehensive backup verification script
cat > backup-verification.sh << 'EOF'
#!/bin/bash
set -e

BACKUP_DATE=${1:-$(date +%Y%m%d)}
NAMESPACE="nephoran-system"
BACKUP_LOCATION="gs://nephoran-backups" # or your backup location

echo "=== BACKUP VERIFICATION FOR $BACKUP_DATE ==="
echo "Started: $(date -u)"

# 1. Verify backup existence and completeness
echo "=== Checking Backup Completeness ==="
backup_files=(
  "kubernetes-state-$BACKUP_DATE.tar.gz"
  "weaviate-data-$BACKUP_DATE.tar.gz"
  "networkintents-$BACKUP_DATE.yaml"
  "secrets-$BACKUP_DATE.yaml"
  "configmaps-$BACKUP_DATE.yaml"
)

for backup_file in "${backup_files[@]}"; do
  if gsutil ls "$BACKUP_LOCATION/$backup_file" >/dev/null 2>&1; then
    size=$(gsutil du "$BACKUP_LOCATION/$backup_file" | awk '{print $1}')
    echo "‚úÖ $backup_file - Size: ${size} bytes"
  else
    echo "‚ùå $backup_file - MISSING"
    exit 1
  fi
done

# 2. Verify backup integrity
echo -e "\n=== Checking Backup Integrity ==="
for backup_file in "${backup_files[@]}"; do
  if [[ "$backup_file" == *.tar.gz ]]; then
    gsutil cp "$BACKUP_LOCATION/$backup_file" "/tmp/$backup_file"
    if tar -tzf "/tmp/$backup_file" >/dev/null 2>&1; then
      echo "‚úÖ $backup_file - Integrity OK"
    else
      echo "‚ùå $backup_file - CORRUPTED"
      exit 1
    fi
    rm "/tmp/$backup_file"
  fi
done

# 3. Verify Weaviate backup content
echo -e "\n=== Verifying Weaviate Backup Content ==="
gsutil cp "$BACKUP_LOCATION/weaviate-data-$BACKUP_DATE.tar.gz" "/tmp/weaviate-backup.tar.gz"
tar -xzf "/tmp/weaviate-backup.tar.gz" -C "/tmp/"

# Check for essential data structures
if [ -d "/tmp/weaviate/data" ]; then
  data_size=$(du -sh "/tmp/weaviate/data" | cut -f1)
  echo "‚úÖ Weaviate data directory - Size: $data_size"
  
  # Check for class data
  classes=$(ls /tmp/weaviate/data/ 2>/dev/null | wc -l)
  echo "‚úÖ Vector classes found: $classes"
else
  echo "‚ùå Weaviate data directory missing"
  exit 1
fi

rm -rf "/tmp/weaviate" "/tmp/weaviate-backup.tar.gz"

# 4. Verify NetworkIntent backup
echo -e "\n=== Verifying NetworkIntent Backup ==="
gsutil cp "$BACKUP_LOCATION/networkintents-$BACKUP_DATE.yaml" "/tmp/ni-backup.yaml"
intent_count=$(grep -c "kind: NetworkIntent" "/tmp/ni-backup.yaml" 2>/dev/null || echo "0")
echo "‚úÖ NetworkIntents backed up: $intent_count"

if [ "$intent_count" -eq 0 ]; then
  echo "‚ö†Ô∏è  Warning: No NetworkIntents in backup (may be expected)"
fi

rm "/tmp/ni-backup.yaml"

# 5. Verify configuration backup
echo -e "\n=== Verifying Configuration Backup ==="
gsutil cp "$BACKUP_LOCATION/configmaps-$BACKUP_DATE.yaml" "/tmp/cm-backup.yaml"
config_count=$(grep -c "kind: ConfigMap" "/tmp/cm-backup.yaml" 2>/dev/null || echo "0")
echo "‚úÖ ConfigMaps backed up: $config_count"

# Check for critical configurations
critical_configs=("llm-processor-config" "rag-api-config" "nephio-bridge-config")
for config in "${critical_configs[@]}"; do
  if grep -q "name: $config" "/tmp/cm-backup.yaml"; then
    echo "‚úÖ Critical config found: $config"
  else
    echo "‚ö†Ô∏è  Warning: Missing critical config: $config"
  fi
done

rm "/tmp/cm-backup.yaml"

# 6. Create verification report
echo -e "\n=== BACKUP VERIFICATION SUMMARY ==="
cat > "backup-verification-report-$BACKUP_DATE.txt" << REPORT
Backup Verification Report
Date: $(date -u)
Backup Date: $BACKUP_DATE
Status: PASSED

Verified Components:
- Kubernetes state backup: ‚úÖ
- Weaviate data backup: ‚úÖ  
- NetworkIntents backup: ‚úÖ
- Configuration backup: ‚úÖ
- Secrets backup: ‚úÖ

Backup Sizes:
$(for file in "${backup_files[@]}"; do gsutil du "$BACKUP_LOCATION/$file" 2>/dev/null || echo "0 $file"; done)

NetworkIntents Count: $intent_count
ConfigMaps Count: $config_count
Weaviate Classes: $classes

Next Verification: $(date -d '+1 day' +%Y%m%d)
REPORT

echo "‚úÖ Backup verification completed successfully"
echo "Report saved: backup-verification-report-$BACKUP_DATE.txt"

# 7. Alert if verification fails
if [ $? -ne 0 ]; then
  echo "‚ùå BACKUP VERIFICATION FAILED"
  # Send alert
  curl -X POST "$SLACK_WEBHOOK_URL" -H 'Content-type: application/json' --data "{
    \"text\": \"üö® BACKUP VERIFICATION FAILED for $BACKUP_DATE\",
    \"attachments\": [{
      \"color\": \"danger\",
      \"text\": \"Immediate attention required. Check backup systems.\"
    }]
  }" 2>/dev/null || echo "Failed to send Slack alert"
  exit 1
fi
EOF

chmod +x backup-verification.sh
```

### Backup Restore Test

```bash
# Monthly backup restore test
cat > backup-restore-test.sh << 'EOF'
#!/bin/bash
set -e

TEST_DATE=$(date +%Y%m%d-%H%M%S)
BACKUP_DATE=${1:-$(date -d '-1 day' +%Y%m%d)}
TEST_NAMESPACE="nephoran-recovery-test"

echo "=== BACKUP RESTORE TEST - $TEST_DATE ==="
echo "Testing backup from: $BACKUP_DATE"

# 1. Create test namespace
echo "Creating test namespace..."
kubectl create namespace "$TEST_NAMESPACE" --dry-run=client -o yaml | kubectl apply -f -

# 2. Restore Weaviate data to test instance
echo "=== Testing Weaviate Restore ==="
gsutil cp "gs://nephoran-backups/weaviate-data-$BACKUP_DATE.tar.gz" "/tmp/test-weaviate.tar.gz"

# Deploy test Weaviate instance
cat <<YAML | kubectl apply -f -
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: weaviate-test
  namespace: $TEST_NAMESPACE
spec:
  serviceName: weaviate-test
  replicas: 1
  selector:
    matchLabels:
      app: weaviate-test
  template:
    metadata:
      labels:
        app: weaviate-test
    spec:
      containers:
      - name: weaviate
        image: semitechnologies/weaviate:latest
        ports:
        - containerPort: 8080
        volumeMounts:
        - name: weaviate-data
          mountPath: /var/lib/weaviate
        env:
        - name: QUERY_DEFAULTS_LIMIT
          value: "100"
        - name: AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED
          value: "true"
        - name: PERSISTENCE_DATA_PATH
          value: "/var/lib/weaviate"
  volumeClaimTemplates:
  - metadata:
      name: weaviate-data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 10Gi
---
apiVersion: v1
kind: Service
metadata:
  name: weaviate-test
  namespace: $TEST_NAMESPACE
spec:
  ports:
  - port: 8080
    targetPort: 8080
  selector:
    app: weaviate-test
YAML

# Wait for Weaviate to be ready
echo "Waiting for test Weaviate to be ready..."
kubectl wait --for=condition=ready pod -l app=weaviate-test -n "$TEST_NAMESPACE" --timeout=300s

# Restore data
echo "Restoring Weaviate data..."
kubectl exec -n "$TEST_NAMESPACE" weaviate-test-0 -- mkdir -p /tmp/restore
kubectl cp "/tmp/test-weaviate.tar.gz" "$TEST_NAMESPACE/weaviate-test-0:/tmp/restore/backup.tar.gz"
kubectl exec -n "$TEST_NAMESPACE" weaviate-test-0 -- sh -c "cd /tmp/restore && tar -xzf backup.tar.gz"
kubectl exec -n "$TEST_NAMESPACE" weaviate-test-0 -- sh -c "cp -r /tmp/restore/weaviate/* /var/lib/weaviate/"
kubectl rollout restart statefulset/weaviate-test -n "$TEST_NAMESPACE"
kubectl wait --for=condition=ready pod -l app=weaviate-test -n "$TEST_NAMESPACE" --timeout=300s

# Test data integrity
echo "Testing Weaviate data integrity..."
kubectl port-forward -n "$TEST_NAMESPACE" svc/weaviate-test 8080:8080 &
PF_PID=$!
sleep 5

# Check if data is accessible
if curl -s "http://localhost:8080/v1/meta" | jq -e '.version' >/dev/null; then
  echo "‚úÖ Weaviate API responsive"
else
  echo "‚ùå Weaviate API not responding"
  kill $PF_PID
  exit 1
fi

# Check schema restoration
schemas=$(curl -s "http://localhost:8080/v1/schema" | jq -r '.classes | length')
echo "‚úÖ Restored schemas: $schemas"

# Check data count
objects=$(curl -s "http://localhost:8080/v1/objects?limit=1" | jq -r '.totalResults // 0')
echo "‚úÖ Restored objects: $objects"

kill $PF_PID

# 3. Test NetworkIntent restoration
echo -e "\n=== Testing NetworkIntent Restore ==="
gsutil cp "gs://nephoran-backups/networkintents-$BACKUP_DATE.yaml" "/tmp/test-intents.yaml"

# Apply to test namespace
sed "s/namespace: default/namespace: $TEST_NAMESPACE/g" "/tmp/test-intents.yaml" > "/tmp/test-intents-namespaced.yaml"
kubectl apply -f "/tmp/test-intents-namespaced.yaml" 2>/dev/null || echo "No NetworkIntents to restore (may be expected)"

# Count restored intents
restored_count=$(kubectl get networkintents -n "$TEST_NAMESPACE" --no-headers 2>/dev/null | wc -l)
echo "‚úÖ NetworkIntents restored: $restored_count"

# 4. Test configuration restoration
echo -e "\n=== Testing Configuration Restore ==="
gsutil cp "gs://nephoran-backups/configmaps-$BACKUP_DATE.yaml" "/tmp/test-configs.yaml"

# Apply configurations to test namespace
sed "s/namespace: nephoran-system/namespace: $TEST_NAMESPACE/g" "/tmp/test-configs.yaml" > "/tmp/test-configs-namespaced.yaml"
kubectl apply -f "/tmp/test-configs-namespaced.yaml"

config_count=$(kubectl get configmaps -n "$TEST_NAMESPACE" --no-headers | wc -l)
echo "‚úÖ ConfigMaps restored: $config_count"

# 5. Generate test report
echo -e "\n=== RESTORE TEST SUMMARY ==="
cat > "restore-test-report-$TEST_DATE.txt" << REPORT
Backup Restore Test Report
Test Date: $(date -u)
Backup Date: $BACKUP_DATE
Test Namespace: $TEST_NAMESPACE
Status: PASSED

Test Results:
- Weaviate Data: ‚úÖ ($objects objects, $schemas schemas)
- NetworkIntents: ‚úÖ ($restored_count intents)
- ConfigMaps: ‚úÖ ($config_count configs)

Performance:
- Weaviate Restore Time: {MEASURE_TIME}
- Data Verification Time: {MEASURE_TIME}
- Total Test Duration: {MEASURE_TIME}

Next Test: $(date -d '+1 month' +%Y%m%d)
REPORT

echo "‚úÖ Restore test completed successfully"
echo "Report saved: restore-test-report-$TEST_DATE.txt"

# 6. Cleanup test resources
echo "Cleaning up test resources..."
kubectl delete namespace "$TEST_NAMESPACE"
rm -f "/tmp/test-*.tar.gz" "/tmp/test-*.yaml"

echo "‚úÖ Test cleanup completed"
EOF

chmod +x backup-restore-test.sh
```

## Failover Procedures

### Multi-Region Failover

```bash
# Regional failover script
cat > regional-failover.sh << 'EOF'
#!/bin/bash
set -e

PRIMARY_REGION=${1:-"us-central1"}
FAILOVER_REGION=${2:-"us-east1"}
FAILOVER_TYPE=${3:-"planned"} # planned or emergency

echo "=== REGIONAL FAILOVER INITIATED ==="
echo "From: $PRIMARY_REGION"
echo "To: $FAILOVER_REGION"
echo "Type: $FAILOVER_TYPE"
echo "Started: $(date -u)"

# 1. Pre-failover checks
echo "=== Pre-Failover Validation ==="

# Check target region readiness
echo "Checking target region cluster..."
kubectl config use-context "gke_project-id_${FAILOVER_REGION}_nephoran-cluster"

if ! kubectl cluster-info >/dev/null 2>&1; then
  echo "‚ùå Target cluster not accessible"
  exit 1
fi

# Check backup availability
LATEST_BACKUP=$(gsutil ls "gs://nephoran-backups/" | grep "$(date +%Y%m%d)" | tail -1)
if [ -z "$LATEST_BACKUP" ]; then
  echo "‚ùå No recent backup available"
  exit 1
fi
echo "‚úÖ Target cluster accessible"
echo "‚úÖ Recent backup available: $LATEST_BACKUP"

# 2. Drain primary region (if planned failover)
if [ "$FAILOVER_TYPE" = "planned" ]; then
  echo -e "\n=== Draining Primary Region ==="
  kubectl config use-context "gke_project-id_${PRIMARY_REGION}_nephoran-cluster"
  
  # Stop accepting new NetworkIntents
  kubectl patch deployment nephoran-controller -n nephoran-system \
    --type='merge' -p='{"spec":{"replicas":0}}'
  
  # Wait for in-progress intents to complete (max 5 minutes)
  echo "Waiting for in-progress NetworkIntents to complete..."
  timeout 300 bash -c 'while kubectl get networkintents -A -o json | jq -e ".items[] | select(.status.phase == \"Processing\")" >/dev/null 2>&1; do sleep 5; done' || echo "Some intents may still be processing"
  
  # Create final backup
  echo "Creating final backup before failover..."
  ./create-backup.sh "pre-failover-$(date +%Y%m%d-%H%M%S)"
fi

# 3. Initialize target region
echo -e "\n=== Initializing Target Region ==="
kubectl config use-context "gke_project-id_${FAILOVER_REGION}_nephoran-cluster"

# Ensure namespace exists
kubectl create namespace nephoran-system --dry-run=client -o yaml | kubectl apply -f -

# Deploy base infrastructure
echo "Deploying Nephoran components..."
helm upgrade --install nephoran-operator deployments/helm/nephoran-operator/ \
  -n nephoran-system \
  --set global.region="$FAILOVER_REGION" \
  --set global.failoverMode=true \
  --wait

# Wait for basic deployment readiness
kubectl wait --for=condition=available --timeout=300s deployment --all -n nephoran-system

# 4. Restore data
echo -e "\n=== Restoring Data ==="

# Restore Weaviate data
echo "Restoring knowledge base..."
BACKUP_DATE=$(date +%Y%m%d)
gsutil cp "gs://nephoran-backups/weaviate-data-$BACKUP_DATE.tar.gz" "/tmp/weaviate-restore.tar.gz"

kubectl exec -n nephoran-system weaviate-0 -- mkdir -p /tmp/restore
kubectl cp "/tmp/weaviate-restore.tar.gz" "nephoran-system/weaviate-0:/tmp/restore/"
kubectl exec -n nephoran-system weaviate-0 -- sh -c "cd /tmp/restore && tar -xzf weaviate-restore.tar.gz"
kubectl exec -n nephoran-system weaviate-0 -- sh -c "cp -r /tmp/restore/weaviate/* /var/lib/weaviate/"

# Restart Weaviate to load restored data
kubectl rollout restart statefulset/weaviate -n nephoran-system
kubectl wait --for=condition=ready pod -l app=weaviate -n nephoran-system --timeout=300s

# Restore NetworkIntents
echo "Restoring NetworkIntents..."
gsutil cp "gs://nephoran-backups/networkintents-$BACKUP_DATE.yaml" "/tmp/networkintents-restore.yaml"
kubectl apply -f "/tmp/networkintents-restore.yaml"

# Restore configurations
echo "Restoring configurations..."
gsutil cp "gs://nephoran-backups/configmaps-$BACKUP_DATE.yaml" "/tmp/configmaps-restore.yaml"
# Filter only nephoran-system configs
grep -A 1000 "namespace: nephoran-system" "/tmp/configmaps-restore.yaml" | kubectl apply -f -

# 5. Update DNS/Load Balancer
echo -e "\n=== Updating Traffic Routing ==="
if [ "$FAILOVER_TYPE" = "planned" ]; then
  # Update DNS to point to failover region
  echo "Update DNS records to point to $FAILOVER_REGION"
  # This would typically integrate with your DNS provider
  echo "Manual action required: Update DNS/Load Balancer configuration"
else
  echo "Emergency failover - DNS should fail over automatically"
fi

# 6. Validation
echo -e "\n=== Post-Failover Validation ==="

# Test basic functionality
echo "Testing basic functionality..."
kubectl port-forward -n nephoran-system svc/nephoran-controller 8081:8081 >/dev/null 2>&1 &
CONTROLLER_PF=$!
sleep 5

if curl -s http://localhost:8081/healthz | jq -e '.status == "healthy"' >/dev/null; then
  echo "‚úÖ Controller health check passed"
else
  echo "‚ùå Controller health check failed"
  kill $CONTROLLER_PF
  exit 1
fi
kill $CONTROLLER_PF

# Test LLM Processor
kubectl port-forward -n nephoran-system svc/llm-processor 8080:8080 >/dev/null 2>&1 &
LLM_PF=$!
sleep 5

if curl -s http://localhost:8080/health | jq -e '.status == "healthy"' >/dev/null; then
  echo "‚úÖ LLM Processor health check passed"
else
  echo "‚ùå LLM Processor health check failed"
  kill $LLM_PF
  exit 1
fi
kill $LLM_PF

# Test Weaviate
kubectl port-forward -n nephoran-system svc/weaviate 8080:8080 >/dev/null 2>&1 &
WEAVIATE_PF=$!
sleep 5

if curl -s http://localhost:8080/v1/meta | jq -e '.version' >/dev/null; then
  echo "‚úÖ Weaviate health check passed"
  objects=$(curl -s http://localhost:8080/v1/objects?limit=1 | jq -r '.totalResults // 0')
  echo "‚úÖ Knowledge base objects: $objects"
else
  echo "‚ùå Weaviate health check failed"
  kill $WEAVIATE_PF
  exit 1
fi
kill $WEAVIATE_PF

# Test NetworkIntent processing
echo "Testing NetworkIntent processing..."
cat <<YAML | kubectl apply -f -
apiVersion: nephoran.com/v1
kind: NetworkIntent
metadata:
  name: failover-test-$(date +%s)
  namespace: default
spec:
  intent: "Deploy test AMF for failover validation"
  priority: low
  scope: cluster
YAML

# Wait for processing
sleep 30
intent_status=$(kubectl get networkintents failover-test-* -o jsonpath='{.items[0].status.phase}' 2>/dev/null || echo "NotFound")
echo "Test intent status: $intent_status"

# 7. Generate failover report
echo -e "\n=== FAILOVER COMPLETED ==="
cat > "failover-report-$(date +%Y%m%d-%H%M%S).txt" << REPORT
Regional Failover Report
Date: $(date -u)
Type: $FAILOVER_TYPE
Primary Region: $PRIMARY_REGION
Failover Region: $FAILOVER_REGION

Status: COMPLETED

Validation Results:
- Controller Health: ‚úÖ
- LLM Processor Health: ‚úÖ
- Weaviate Health: ‚úÖ
- Knowledge Base Objects: $objects
- Test Intent Processing: $intent_status

Next Steps:
- Monitor system stability for 24 hours
- Update documentation with new region info
- Plan failback when primary region is restored
REPORT

echo "‚úÖ Regional failover completed successfully"
echo "Report saved: failover-report-$(date +%Y%m%d-%H%M%S).txt"

# 8. Cleanup
rm -f /tmp/*restore*
EOF

chmod +x regional-failover.sh
```

### Component-Level Failover

```bash
# Individual component failover
cat > component-failover.sh << 'EOF'
#!/bin/bash
set -e

COMPONENT=$1
FAILOVER_TYPE=${2:-"restart"} # restart, restore, or rebuild

echo "=== COMPONENT FAILOVER: $COMPONENT ==="
echo "Type: $FAILOVER_TYPE"
echo "Started: $(date -u)"

case "$COMPONENT" in
  "weaviate")
    echo "=== Weaviate Failover ==="
    
    if [ "$FAILOVER_TYPE" = "restart" ]; then
      kubectl rollout restart statefulset/weaviate -n nephoran-system
      kubectl wait --for=condition=ready pod -l app=weaviate -n nephoran-system --timeout=300s
      
    elif [ "$FAILOVER_TYPE" = "restore" ]; then
      # Restore from backup
      BACKUP_DATE=$(date +%Y%m%d)
      gsutil cp "gs://nephoran-backups/weaviate-data-$BACKUP_DATE.tar.gz" "/tmp/weaviate-restore.tar.gz"
      
      # Scale down
      kubectl scale statefulset weaviate --replicas=0 -n nephoran-system
      kubectl wait --for=delete pod -l app=weaviate -n nephoran-system --timeout=300s
      
      # Restore data
      kubectl scale statefulset weaviate --replicas=1 -n nephoran-system
      kubectl wait --for=condition=ready pod -l app=weaviate -n nephoran-system --timeout=300s
      
      kubectl cp "/tmp/weaviate-restore.tar.gz" "nephoran-system/weaviate-0:/tmp/"
      kubectl exec -n nephoran-system weaviate-0 -- sh -c "cd /tmp && tar -xzf weaviate-restore.tar.gz"
      kubectl exec -n nephoran-system weaviate-0 -- sh -c "cp -r /tmp/weaviate/* /var/lib/weaviate/"
      kubectl rollout restart statefulset/weaviate -n nephoran-system
      kubectl wait --for=condition=ready pod -l app=weaviate -n nephoran-system --timeout=300s
    fi
    
    # Validate
    kubectl port-forward -n nephoran-system svc/weaviate 8080:8080 &
    PF_PID=$!
    sleep 5
    if curl -s http://localhost:8080/v1/meta | jq -e '.version' >/dev/null; then
      echo "‚úÖ Weaviate failover successful"
    else
      echo "‚ùå Weaviate failover failed"
      kill $PF_PID
      exit 1
    fi
    kill $PF_PID
    ;;
    
  "llm-processor")
    echo "=== LLM Processor Failover ==="
    
    if [ "$FAILOVER_TYPE" = "restart" ]; then
      kubectl rollout restart deployment/llm-processor -n nephoran-system
      kubectl wait --for=condition=available deployment/llm-processor -n nephoran-system --timeout=300s
    elif [ "$FAILOVER_TYPE" = "scale" ]; then
      current_replicas=$(kubectl get deployment llm-processor -n nephoran-system -o jsonpath='{.spec.replicas}')
      kubectl scale deployment llm-processor --replicas=$((current_replicas + 1)) -n nephoran-system
    fi
    
    # Validate
    kubectl port-forward -n nephoran-system svc/llm-processor 8080:8080 &
    PF_PID=$!
    sleep 5
    if curl -s http://localhost:8080/health | jq -e '.status == "healthy"' >/dev/null; then
      echo "‚úÖ LLM Processor failover successful"
    else
      echo "‚ùå LLM Processor failover failed"
      kill $PF_PID
      exit 1
    fi
    kill $PF_PID
    ;;
    
  "controller")
    echo "=== Controller Failover ==="
    
    if [ "$FAILOVER_TYPE" = "restart" ]; then
      kubectl rollout restart deployment/nephoran-controller -n nephoran-system
      kubectl wait --for=condition=available deployment/nephoran-controller -n nephoran-system --timeout=300s
    fi
    
    # Validate
    kubectl port-forward -n nephoran-system svc/nephoran-controller 8081:8081 &
    PF_PID=$!
    sleep 5
    if curl -s http://localhost:8081/healthz | jq -e '.status == "healthy"' >/dev/null; then
      echo "‚úÖ Controller failover successful"
    else
      echo "‚ùå Controller failover failed"
      kill $PF_PID
      exit 1
    fi
    kill $PF_PID
    ;;
    
  *)
    echo "‚ùå Unknown component: $COMPONENT"
    echo "Supported components: weaviate, llm-processor, controller"
    exit 1
    ;;
esac

echo "‚úÖ Component failover completed: $COMPONENT"
EOF

chmod +x component-failover.sh
```

## Data Restoration Steps

### Complete System Restoration

```bash
# Full system restore from backup
cat > full-system-restore.sh << 'EOF'
#!/bin/bash
set -e

RESTORE_DATE=${1:-$(date +%Y%m%d)}
TARGET_CLUSTER=${2:-"current"}
NAMESPACE="nephoran-system"

echo "=== FULL SYSTEM RESTORATION ==="
echo "Restore Date: $RESTORE_DATE"
echo "Target Cluster: $TARGET_CLUSTER"
echo "Started: $(date -u)"

# 1. Pre-restoration validation
echo "=== Pre-Restoration Validation ==="

# Check backup availability
echo "Checking backup availability..."
backup_files=(
  "kubernetes-state-$RESTORE_DATE.tar.gz"
  "weaviate-data-$RESTORE_DATE.tar.gz"
  "networkintents-$RESTORE_DATE.yaml"
  "secrets-$RESTORE_DATE.yaml"
  "configmaps-$RESTORE_DATE.yaml"
)

for backup_file in "${backup_files[@]}"; do
  if gsutil ls "gs://nephoran-backups/$backup_file" >/dev/null 2>&1; then
    echo "‚úÖ Found: $backup_file"
  else
    echo "‚ùå Missing: $backup_file"
    exit 1
  fi
done

# Check cluster readiness
if ! kubectl cluster-info >/dev/null 2>&1; then
  echo "‚ùå Target cluster not accessible"
  exit 1
fi
echo "‚úÖ Target cluster accessible"

# 2. Stop existing services (if any)
echo -e "\n=== Stopping Existing Services ==="
kubectl delete namespace "$NAMESPACE" --ignore-not-found=true --wait=true
kubectl create namespace "$NAMESPACE"

# 3. Restore Kubernetes state
echo -e "\n=== Restoring Kubernetes State ==="
gsutil cp "gs://nephoran-backups/kubernetes-state-$RESTORE_DATE.tar.gz" "/tmp/k8s-state.tar.gz"
tar -xzf "/tmp/k8s-state.tar.gz" -C "/tmp/"

# Apply CRDs first
if [ -d "/tmp/kubernetes-state/crds" ]; then
  echo "Restoring CRDs..."
  kubectl apply -f "/tmp/kubernetes-state/crds/"
fi

# Apply RBAC
if [ -d "/tmp/kubernetes-state/rbac" ]; then
  echo "Restoring RBAC..."
  kubectl apply -f "/tmp/kubernetes-state/rbac/"
fi

# 4. Restore secrets
echo -e "\n=== Restoring Secrets ==="
gsutil cp "gs://nephoran-backups/secrets-$RESTORE_DATE.yaml" "/tmp/secrets-restore.yaml"
kubectl apply -f "/tmp/secrets-restore.yaml" -n "$NAMESPACE"

# 5. Restore configurations
echo -e "\n=== Restoring Configuration ==="
gsutil cp "gs://nephoran-backups/configmaps-$RESTORE_DATE.yaml" "/tmp/configmaps-restore.yaml"
kubectl apply -f "/tmp/configmaps-restore.yaml" -n "$NAMESPACE"

# 6. Deploy core infrastructure
echo -e "\n=== Deploying Core Infrastructure ==="
helm upgrade --install nephoran-operator deployments/helm/nephoran-operator/ \
  -n "$NAMESPACE" \
  --set global.restoreMode=true \
  --wait --timeout=600s

# Wait for base services to be ready
kubectl wait --for=condition=available --timeout=300s deployment --all -n "$NAMESPACE"

# 7. Restore Weaviate data
echo -e "\n=== Restoring Weaviate Data ==="
gsutil cp "gs://nephoran-backups/weaviate-data-$RESTORE_DATE.tar.gz" "/tmp/weaviate-restore.tar.gz"

# Wait for Weaviate to be ready first
kubectl wait --for=condition=ready pod -l app=weaviate -n "$NAMESPACE" --timeout=300s

# Scale down Weaviate for data restoration
kubectl scale statefulset weaviate --replicas=0 -n "$NAMESPACE"
kubectl wait --for=delete pod -l app=weaviate -n "$NAMESPACE" --timeout=300s

# Scale back up
kubectl scale statefulset weaviate --replicas=1 -n "$NAMESPACE"
kubectl wait --for=condition=ready pod -l app=weaviate -n "$NAMESPACE" --timeout=300s

# Restore data
kubectl cp "/tmp/weaviate-restore.tar.gz" "$NAMESPACE/weaviate-0:/tmp/"
kubectl exec -n "$NAMESPACE" weaviate-0 -- sh -c "cd /tmp && tar -xzf weaviate-restore.tar.gz"
kubectl exec -n "$NAMESPACE" weaviate-0 -- sh -c "rm -rf /var/lib/weaviate/* && cp -r /tmp/weaviate/* /var/lib/weaviate/"

# Restart Weaviate to load restored data
kubectl rollout restart statefulset/weaviate -n "$NAMESPACE"
kubectl wait --for=condition=ready pod -l app=weaviate -n "$NAMESPACE" --timeout=300s

# 8. Restore NetworkIntents
echo -e "\n=== Restoring NetworkIntents ==="
gsutil cp "gs://nephoran-backups/networkintents-$RESTORE_DATE.yaml" "/tmp/networkintents-restore.yaml"
kubectl apply -f "/tmp/networkintents-restore.yaml"

# 9. Restore custom resources
echo -e "\n=== Restoring Custom Resources ==="
if gsutil ls "gs://nephoran-backups/e2nodesets-$RESTORE_DATE.yaml" >/dev/null 2>&1; then
  gsutil cp "gs://nephoran-backups/e2nodesets-$RESTORE_DATE.yaml" "/tmp/e2nodesets-restore.yaml"
  kubectl apply -f "/tmp/e2nodesets-restore.yaml"
fi

# 10. Final system validation
echo -e "\n=== System Validation ==="

# Wait for all components to be ready
echo "Waiting for all services to be ready..."
kubectl wait --for=condition=available --timeout=600s deployment --all -n "$NAMESPACE"

# Validate each component
components=("nephoran-controller" "llm-processor" "rag-api" "weaviate" "nephio-bridge")
for component in "${components[@]}"; do
  if kubectl get pods -n "$NAMESPACE" -l "app=$component" | grep -q Running; then
    echo "‚úÖ $component: Running"
  else
    echo "‚ùå $component: Not running"
    kubectl get pods -n "$NAMESPACE" -l "app=$component"
  fi
done

# Test basic functionality
echo "Testing basic functionality..."

# Test controller
kubectl port-forward -n "$NAMESPACE" svc/nephoran-controller 8081:8081 &
CONTROLLER_PF=$!
sleep 5
if curl -s http://localhost:8081/healthz | jq -e '.status == "healthy"' >/dev/null; then
  echo "‚úÖ Controller health check passed"
else
  echo "‚ùå Controller health check failed"
fi
kill $CONTROLLER_PF

# Test Weaviate data
kubectl port-forward -n "$NAMESPACE" svc/weaviate 8080:8080 &
WEAVIATE_PF=$!
sleep 5
if curl -s http://localhost:8080/v1/meta | jq -e '.version' >/dev/null; then
  objects=$(curl -s http://localhost:8080/v1/objects?limit=1 | jq -r '.totalResults // 0')
  echo "‚úÖ Weaviate data restored: $objects objects"
else
  echo "‚ùå Weaviate data restoration failed"
fi
kill $WEAVIATE_PF

# Count restored resources
ni_count=$(kubectl get networkintents -A --no-headers 2>/dev/null | wc -l)
cm_count=$(kubectl get configmaps -n "$NAMESPACE" --no-headers 2>/dev/null | wc -l)
secret_count=$(kubectl get secrets -n "$NAMESPACE" --no-headers 2>/dev/null | wc -l)

echo "‚úÖ NetworkIntents restored: $ni_count"
echo "‚úÖ ConfigMaps restored: $cm_count"
echo "‚úÖ Secrets restored: $secret_count"

# 11. Generate restoration report
echo -e "\n=== RESTORATION COMPLETED ==="
cat > "restoration-report-$(date +%Y%m%d-%H%M%S).txt" << REPORT
Full System Restoration Report
Date: $(date -u)
Restore Date: $RESTORE_DATE
Target Cluster: $TARGET_CLUSTER
Namespace: $NAMESPACE

Status: COMPLETED

Restored Components:
- Kubernetes State: ‚úÖ
- Secrets: ‚úÖ ($secret_count secrets)
- ConfigMaps: ‚úÖ ($cm_count configs)
- Weaviate Data: ‚úÖ ($objects objects)
- NetworkIntents: ‚úÖ ($ni_count intents)
- Custom Resources: ‚úÖ

Component Status:
$(for comp in "${components[@]}"; do echo "- $comp: $(kubectl get pods -n "$NAMESPACE" -l "app=$comp" -o jsonpath='{.items[0].status.phase}' 2>/dev/null || echo 'Not Found')"; done)

Next Steps:
- Monitor system stability for 24 hours
- Run integration tests
- Update monitoring baselines
- Document any configuration changes

Restoration Duration: {CALCULATE_DURATION}
REPORT

echo "‚úÖ Full system restoration completed successfully"
echo "Report saved: restoration-report-$(date +%Y%m%d-%H%M%S).txt"

# 12. Cleanup temporary files
rm -rf /tmp/kubernetes-state/ /tmp/*restore*

# 13. Create test NetworkIntent to verify functionality
echo "Creating test NetworkIntent to verify functionality..."
cat <<YAML | kubectl apply -f -
apiVersion: nephoran.com/v1
kind: NetworkIntent
metadata:
  name: restore-test-$(date +%s)
  namespace: default
spec:
  intent: "Deploy test AMF to verify restoration"
  priority: low
  scope: cluster
YAML

echo "‚úÖ Test NetworkIntent created. Monitor processing to confirm full functionality."
EOF

chmod +x full-system-restore.sh
```

## Service Validation

### Post-Recovery Validation Suite

```bash
# Comprehensive validation after recovery
cat > post-recovery-validation.sh << 'EOF'
#!/bin/bash
set -e

VALIDATION_ID="VAL-$(date +%Y%m%d-%H%M%S)"
NAMESPACE="nephoran-system"
RESULTS_DIR="/tmp/validation-$VALIDATION_ID"
mkdir -p "$RESULTS_DIR"

echo "=== POST-RECOVERY VALIDATION SUITE ==="
echo "Validation ID: $VALIDATION_ID"
echo "Started: $(date -u)"

# Initialize validation report
cat > "$RESULTS_DIR/validation-report.txt" << HEADER
Post-Recovery Validation Report
Validation ID: $VALIDATION_ID
Date: $(date -u)
Status: IN PROGRESS

HEADER

# 1. Infrastructure Health Checks
echo "=== Infrastructure Health Validation ==="
echo "## Infrastructure Health" >> "$RESULTS_DIR/validation-report.txt"

# Cluster health
if kubectl cluster-info >/dev/null 2>&1; then
  echo "‚úÖ Cluster connectivity" | tee -a "$RESULTS_DIR/validation-report.txt"
else
  echo "‚ùå Cluster connectivity" | tee -a "$RESULTS_DIR/validation-report.txt"
  exit 1
fi

# Node health
unhealthy_nodes=$(kubectl get nodes --no-headers | grep -v " Ready " | wc -l)
if [ "$unhealthy_nodes" -eq 0 ]; then
  echo "‚úÖ All nodes healthy" | tee -a "$RESULTS_DIR/validation-report.txt"
else
  echo "‚ùå $unhealthy_nodes unhealthy nodes" | tee -a "$RESULTS_DIR/validation-report.txt"
  kubectl get nodes >> "$RESULTS_DIR/validation-report.txt"
fi

# Namespace health
if kubectl get namespace "$NAMESPACE" >/dev/null 2>&1; then
  echo "‚úÖ Namespace exists" | tee -a "$RESULTS_DIR/validation-report.txt"
else
  echo "‚ùå Namespace missing" | tee -a "$RESULTS_DIR/validation-report.txt"
  exit 1
fi

# 2. Component Health Validation
echo -e "\n=== Component Health Validation ==="
echo -e "\n## Component Health" >> "$RESULTS_DIR/validation-report.txt"

components=("nephoran-controller" "llm-processor" "rag-api" "weaviate" "nephio-bridge")
for component in "${components[@]}"; do
  echo "Validating $component..."
  
  # Pod status
  pod_status=$(kubectl get pods -n "$NAMESPACE" -l "app=$component" -o jsonpath='{.items[0].status.phase}' 2>/dev/null || echo "NotFound")
  
  if [ "$pod_status" = "Running" ]; then
    # Check readiness
    ready=$(kubectl get pods -n "$NAMESPACE" -l "app=$component" -o jsonpath='{.items[0].status.conditions[?(@.type=="Ready")].status}' 2>/dev/null || echo "False")
    if [ "$ready" = "True" ]; then
      echo "‚úÖ $component: Running and Ready" | tee -a "$RESULTS_DIR/validation-report.txt"
    else
      echo "‚ö†Ô∏è  $component: Running but Not Ready" | tee -a "$RESULTS_DIR/validation-report.txt"
    fi
  else
    echo "‚ùå $component: $pod_status" | tee -a "$RESULTS_DIR/validation-report.txt"
  fi
  
  # Get detailed pod info
  kubectl describe pod -n "$NAMESPACE" -l "app=$component" > "$RESULTS_DIR/${component}-pod-details.txt" 2>/dev/null || echo "No pods found for $component" > "$RESULTS_DIR/${component}-pod-details.txt"
done

# 3. API Health Validation
echo -e "\n=== API Health Validation ==="
echo -e "\n## API Health" >> "$RESULTS_DIR/validation-report.txt"

# Controller API
kubectl port-forward -n "$NAMESPACE" svc/nephoran-controller 8081:8081 >/dev/null 2>&1 &
CONTROLLER_PF=$!
sleep 5

if curl -s http://localhost:8081/healthz >/dev/null 2>&1; then
  health_status=$(curl -s http://localhost:8081/healthz | jq -r '.status' 2>/dev/null || echo "unknown")
  echo "‚úÖ Controller API: $health_status" | tee -a "$RESULTS_DIR/validation-report.txt"
  curl -s http://localhost:8081/healthz > "$RESULTS_DIR/controller-health.json"
else
  echo "‚ùå Controller API: Unreachable" | tee -a "$RESULTS_DIR/validation-report.txt"
fi
kill $CONTROLLER_PF 2>/dev/null

# LLM Processor API
kubectl port-forward -n "$NAMESPACE" svc/llm-processor 8080:8080 >/dev/null 2>&1 &
LLM_PF=$!
sleep 5

if curl -s http://localhost:8080/health >/dev/null 2>&1; then
  llm_status=$(curl -s http://localhost:8080/health | jq -r '.status' 2>/dev/null || echo "unknown")
  echo "‚úÖ LLM Processor API: $llm_status" | tee -a "$RESULTS_DIR/validation-report.txt"
  curl -s http://localhost:8080/health > "$RESULTS_DIR/llm-health.json"
  curl -s http://localhost:8080/admin/circuit-breaker/status > "$RESULTS_DIR/circuit-breaker-status.json" 2>/dev/null
else
  echo "‚ùå LLM Processor API: Unreachable" | tee -a "$RESULTS_DIR/validation-report.txt"
fi
kill $LLM_PF 2>/dev/null

# RAG API
kubectl port-forward -n "$NAMESPACE" svc/rag-api 8082:8080 >/dev/null 2>&1 &
RAG_PF=$!
sleep 5

if curl -s http://localhost:8082/health >/dev/null 2>&1; then
  rag_status=$(curl -s http://localhost:8082/health | jq -r '.status' 2>/dev/null || echo "unknown")
  echo "‚úÖ RAG API: $rag_status" | tee -a "$RESULTS_DIR/validation-report.txt"
  curl -s http://localhost:8082/health > "$RESULTS_DIR/rag-health.json"
else
  echo "‚ùå RAG API: Unreachable" | tee -a "$RESULTS_DIR/validation-report.txt"
fi
kill $RAG_PF 2>/dev/null

# Weaviate API
kubectl port-forward -n "$NAMESPACE" svc/weaviate 8080:8080 >/dev/null 2>&1 &
WEAVIATE_PF=$!
sleep 5

if curl -s http://localhost:8080/v1/meta >/dev/null 2>&1; then
  weaviate_version=$(curl -s http://localhost:8080/v1/meta | jq -r '.version' 2>/dev/null || echo "unknown")
  echo "‚úÖ Weaviate API: $weaviate_version" | tee -a "$RESULTS_DIR/validation-report.txt"
  curl -s http://localhost:8080/v1/meta > "$RESULTS_DIR/weaviate-meta.json"
  curl -s http://localhost:8080/v1/schema > "$RESULTS_DIR/weaviate-schema.json"
else
  echo "‚ùå Weaviate API: Unreachable" | tee -a "$RESULTS_DIR/validation-report.txt"
fi
kill $WEAVIATE_PF 2>/dev/null

# 4. Data Integrity Validation
echo -e "\n=== Data Integrity Validation ==="
echo -e "\n## Data Integrity" >> "$RESULTS_DIR/validation-report.txt"

# NetworkIntents count
ni_count=$(kubectl get networkintents -A --no-headers 2>/dev/null | wc -l)
echo "‚úÖ NetworkIntents: $ni_count" | tee -a "$RESULTS_DIR/validation-report.txt"

# Weaviate data
kubectl port-forward -n "$NAMESPACE" svc/weaviate 8080:8080 >/dev/null 2>&1 &
WEAVIATE_PF=$!
sleep 5

if curl -s http://localhost:8080/v1/objects?limit=1 >/dev/null 2>&1; then
  object_count=$(curl -s http://localhost:8080/v1/objects?limit=1 | jq -r '.totalResults // 0')
  schema_count=$(curl -s http://localhost:8080/v1/schema | jq -r '.classes | length')
  echo "‚úÖ Weaviate Objects: $object_count" | tee -a "$RESULTS_DIR/validation-report.txt"
  echo "‚úÖ Weaviate Schemas: $schema_count" | tee -a "$RESULTS_DIR/validation-report.txt"
else
  echo "‚ùå Weaviate data check failed" | tee -a "$RESULTS_DIR/validation-report.txt"
fi
kill $WEAVIATE_PF 2>/dev/null

# ConfigMaps and Secrets
cm_count=$(kubectl get configmaps -n "$NAMESPACE" --no-headers 2>/dev/null | wc -l)
secret_count=$(kubectl get secrets -n "$NAMESPACE" --no-headers 2>/dev/null | wc -l)
echo "‚úÖ ConfigMaps: $cm_count" | tee -a "$RESULTS_DIR/validation-report.txt"
echo "‚úÖ Secrets: $secret_count" | tee -a "$RESULTS_DIR/validation-report.txt"

# 5. Functional Testing
echo -e "\n=== Functional Testing ==="
echo -e "\n## Functional Tests" >> "$RESULTS_DIR/validation-report.txt"

# Create test NetworkIntent
test_intent_name="validation-test-$(date +%s)"
cat <<YAML | kubectl apply -f -
apiVersion: nephoran.com/v1
kind: NetworkIntent
metadata:
  name: $test_intent_name
  namespace: default
  labels:
    test-type: post-recovery-validation
spec:
  intent: "Deploy test AMF for post-recovery validation"
  priority: low
  scope: cluster
YAML

echo "Created test NetworkIntent: $test_intent_name"

# Wait for processing
echo "Waiting for NetworkIntent processing (max 2 minutes)..."
timeout=120
elapsed=0
while [ $elapsed -lt $timeout ]; do
  status=$(kubectl get networkintent "$test_intent_name" -n default -o jsonpath='{.status.phase}' 2>/dev/null || echo "NotFound")
  if [ "$status" != "Processing" ] && [ "$status" != "Pending" ] && [ "$status" != "" ]; then
    break
  fi
  sleep 5
  elapsed=$((elapsed + 5))
done

final_status=$(kubectl get networkintent "$test_intent_name" -n default -o jsonpath='{.status.phase}' 2>/dev/null || echo "NotFound")
echo "Test NetworkIntent final status: $final_status" | tee -a "$RESULTS_DIR/validation-report.txt"

if [ "$final_status" = "Completed" ]; then
  echo "‚úÖ NetworkIntent processing functional" | tee -a "$RESULTS_DIR/validation-report.txt"
else
  echo "‚ùå NetworkIntent processing failed: $final_status" | tee -a "$RESULTS_DIR/validation-report.txt"
  kubectl describe networkintent "$test_intent_name" -n default > "$RESULTS_DIR/test-intent-details.txt"
fi

# Test knowledge retrieval
kubectl port-forward -n "$NAMESPACE" svc/rag-api 8082:8080 >/dev/null 2>&1 &
RAG_PF=$!
sleep 5

if curl -s -X POST http://localhost:8082/v1/query \
  -H "Content-Type: application/json" \
  -d '{"query":"test knowledge retrieval","top_k":3}' >/dev/null 2>&1; then
  results=$(curl -s -X POST http://localhost:8082/v1/query \
    -H "Content-Type: application/json" \
    -d '{"query":"test knowledge retrieval","top_k":3}' | jq -r '.results | length' 2>/dev/null || echo "0")
  echo "‚úÖ Knowledge retrieval functional: $results results" | tee -a "$RESULTS_DIR/validation-report.txt"
else
  echo "‚ùå Knowledge retrieval failed" | tee -a "$RESULTS_DIR/validation-report.txt"
fi
kill $RAG_PF 2>/dev/null

# 6. Performance Validation
echo -e "\n=== Performance Validation ==="
echo -e "\n## Performance Metrics" >> "$RESULTS_DIR/validation-report.txt"

# Resource usage
kubectl top pods -n "$NAMESPACE" > "$RESULTS_DIR/resource-usage.txt" 2>/dev/null || echo "Metrics server not available"
if [ -f "$RESULTS_DIR/resource-usage.txt" ]; then
  echo "‚úÖ Resource usage captured" | tee -a "$RESULTS_DIR/validation-report.txt"
  cat "$RESULTS_DIR/resource-usage.txt" >> "$RESULTS_DIR/validation-report.txt"
fi

# 7. Generate final report
echo -e "\n=== VALIDATION COMPLETED ==="

# Count pass/fail results
passes=$(grep -c "‚úÖ" "$RESULTS_DIR/validation-report.txt" || echo "0")
failures=$(grep -c "‚ùå" "$RESULTS_DIR/validation-report.txt" || echo "0")
warnings=$(grep -c "‚ö†Ô∏è" "$RESULTS_DIR/validation-report.txt" || echo "0")

# Update report status
sed -i 's/Status: IN PROGRESS/Status: COMPLETED/' "$RESULTS_DIR/validation-report.txt"

cat >> "$RESULTS_DIR/validation-report.txt" << SUMMARY

## Validation Summary
Total Checks: $((passes + failures + warnings))
Passed: $passes
Failed: $failures
Warnings: $warnings

## Overall Status
$(if [ "$failures" -eq 0 ]; then echo "‚úÖ VALIDATION PASSED"; else echo "‚ùå VALIDATION FAILED"; fi)

Generated: $(date -u)
Duration: {CALCULATE_DURATION}
SUMMARY

if [ "$failures" -eq 0 ]; then
  echo "‚úÖ Post-recovery validation PASSED ($passes checks)"
  echo "Report saved: $RESULTS_DIR/validation-report.txt"
  exit 0
else
  echo "‚ùå Post-recovery validation FAILED ($failures failures, $passes passes)"
  echo "Report saved: $RESULTS_DIR/validation-report.txt"
  echo "Review failed checks and take corrective action."
  exit 1
fi

# Cleanup test resources
kubectl delete networkintent "$test_intent_name" -n default --ignore-not-found=true
EOF

chmod +x post-recovery-validation.sh
```

## Business Continuity

### Communication During Disaster

```bash
# Business continuity communication plan
cat > disaster-communication.sh << 'EOF'
#!/bin/bash
set -e

DISASTER_TYPE=$1
SEVERITY=$2
ESTIMATED_DURATION=$3
INCIDENT_ID="DR-$(date +%Y%m%d-%H%M%S)"

echo "=== DISASTER RECOVERY COMMUNICATION INITIATED ==="
echo "Incident ID: $INCIDENT_ID"
echo "Type: $DISASTER_TYPE"
echo "Severity: $SEVERITY"

# Communication templates by audience
case "$SEVERITY" in
  "P1"|"CRITICAL")
    # Executive notification
    cat > "executive-notification-$INCIDENT_ID.md" << EXEC
# CRITICAL: Disaster Recovery in Progress

**Incident ID**: $INCIDENT_ID
**Type**: $DISASTER_TYPE
**Severity**: Critical (P1)
**Status**: Disaster Recovery Initiated
**Start Time**: $(date -u)

## Business Impact
- Nephoran Intent Operator services unavailable
- Network intent processing stopped
- Estimated restoration time: $ESTIMATED_DURATION

## Actions Being Taken
- Disaster recovery team activated
- Backup systems being deployed
- Customer notification initiated
- Regular updates every 30 minutes

## Communication Plan
- Next update: $(date -u -d '+30 minutes')
- Stakeholder calls scheduled hourly
- Customer support briefed

**Incident Commander**: [Name]
**Contact**: [Phone/Email]
EXEC

    # Customer notification
    cat > "customer-notification-$INCIDENT_ID.md" << CUSTOMER
# Service Interruption - Immediate Action Required

We are experiencing a critical system issue that has temporarily interrupted Nephoran Intent Operator services.

## Current Status
- **Issue**: $DISASTER_TYPE
- **Impact**: Service unavailable
- **Start Time**: $(date -u)

## What We're Doing
- Emergency response team activated
- Backup systems being deployed
- Working around the clock for resolution

## What You Should Do
- Hold any new NetworkIntent deployments
- Contact support for urgent requirements
- Check status page for updates: [URL]

## Expected Resolution
We expect service restoration within $ESTIMATED_DURATION.

Updates will be provided every 30 minutes.

We sincerely apologize for this interruption and are working diligently to restore service.
CUSTOMER

    # Internal team alert
    cat > "team-alert-$INCIDENT_ID.md" << TEAM
# üö® CRITICAL DISASTER RECOVERY - ALL HANDS

**Incident**: $INCIDENT_ID
**Type**: $DISASTER_TYPE
**Status**: Active Response

## Immediate Actions Required
- [ ] All engineering on-call activated
- [ ] Disaster recovery procedures initiated
- [ ] Customer support briefed
- [ ] Management notifications sent

## War Room
- **Location**: [Virtual meeting link]
- **Bridge**: [Conference number]
- **Slack**: #disaster-recovery-$INCIDENT_ID

## Key Contacts
- Incident Commander: [Name/Phone]
- Engineering Lead: [Name/Phone]
- Customer Success: [Name/Phone]

**STATUS**: All hands on deck until resolved
TEAM
    ;;
    
  "P2"|"HIGH")
    # Stakeholder notification
    cat > "stakeholder-notification-$INCIDENT_ID.md" << STAKEHOLDER
# Service Degradation - Recovery in Progress

**Incident ID**: $INCIDENT_ID
**Type**: $DISASTER_TYPE
**Severity**: High (P2)
**Impact**: Partial service unavailability

## Current Situation
- Started: $(date -u)
- Impact: $DISASTER_TYPE
- Recovery: In progress

## Actions Taken
- Response team activated
- Recovery procedures initiated
- Customers notified
- Regular monitoring increased

## Timeline
- Next update: $(date -u -d '+1 hour')
- Expected resolution: $ESTIMATED_DURATION

**Point of Contact**: [Name/Email]
STAKEHOLDER
    ;;
esac

# Send notifications based on severity
if [ "$SEVERITY" = "P1" ] || [ "$SEVERITY" = "CRITICAL" ]; then
  echo "Sending CRITICAL disaster notifications..."
  
  # Send executive notification
  echo "üìß Executive notification prepared: executive-notification-$INCIDENT_ID.md"
  
  # Send customer notification
  echo "üìß Customer notification prepared: customer-notification-$INCIDENT_ID.md"
  
  # Send team alert
  echo "üìß Team alert prepared: team-alert-$INCIDENT_ID.md"
  
  # Slack notifications
  if [ -n "$SLACK_WEBHOOK_URL" ]; then
    curl -X POST "$SLACK_WEBHOOK_URL" \
      -H 'Content-type: application/json' \
      --data "{
        \"text\": \"üö® DISASTER RECOVERY INITIATED\",
        \"attachments\": [{
          \"color\": \"danger\",
          \"fields\": [
            {\"title\": \"Incident ID\", \"value\": \"$INCIDENT_ID\", \"short\": true},
            {\"title\": \"Type\", \"value\": \"$DISASTER_TYPE\", \"short\": true},
            {\"title\": \"Severity\", \"value\": \"$SEVERITY\", \"short\": true},
            {\"title\": \"ETA\", \"value\": \"$ESTIMATED_DURATION\", \"short\": true}
          ],
          \"actions\": [{
            \"type\": \"button\",
            \"text\": \"Join War Room\",
            \"url\": \"[WAR_ROOM_LINK]\"
          }]
        }]
      }"
  fi
  
  # Status page update
  echo "üåê Update status page to 'Major Outage'"
  
  # PagerDuty escalation
  echo "üì± PagerDuty critical alert sent"
  
elif [ "$SEVERITY" = "P2" ] || [ "$SEVERITY" = "HIGH" ]; then
  echo "Sending HIGH severity disaster notifications..."
  echo "üìß Stakeholder notification prepared: stakeholder-notification-$INCIDENT_ID.md"
  
  # Status page update
  echo "üåê Update status page to 'Partial Outage'"
fi

echo "‚úÖ Disaster recovery communication initiated"
echo "Files generated:"
ls -la *notification*$INCIDENT_ID* 2>/dev/null || echo "No notification files generated"
ls -la *alert*$INCIDENT_ID* 2>/dev/null || echo "No alert files generated"
EOF

chmod +x disaster-communication.sh
```

## Recovery Testing

### Disaster Recovery Testing Framework

```bash
# Comprehensive DR testing framework
cat > dr-testing-framework.sh << 'EOF'
#!/bin/bash
set -e

TEST_TYPE=${1:-"component"} # component, regional, full
TEST_ENV=${2:-"staging"}    # staging, production-like
DRY_RUN=${3:-"false"}       # true for validation only

echo "=== DISASTER RECOVERY TESTING FRAMEWORK ==="
echo "Test Type: $TEST_TYPE"
echo "Environment: $TEST_ENV"
echo "Dry Run: $DRY_RUN"
echo "Started: $(date -u)"

TEST_ID="DRTEST-$(date +%Y%m%d-%H%M%S)"
TEST_DIR="/tmp/dr-test-$TEST_ID"
mkdir -p "$TEST_DIR"

# Initialize test report
cat > "$TEST_DIR/test-report.md" << HEADER
# Disaster Recovery Test Report

**Test ID**: $TEST_ID
**Type**: $TEST_TYPE
**Environment**: $TEST_ENV
**Date**: $(date -u)
**Status**: IN PROGRESS

HEADER

case "$TEST_TYPE" in
  "component")
    echo "=== COMPONENT FAILURE TESTING ==="
    echo -e "\n## Component Failure Tests" >> "$TEST_DIR/test-report.md"
    
    components=("weaviate" "llm-processor" "rag-api" "nephoran-controller")
    for component in "${components[@]}"; do
      echo "Testing $component failure and recovery..."
      
      if [ "$DRY_RUN" = "false" ]; then
        # Simulate component failure
        kubectl scale deployment "$component" --replicas=0 -n nephoran-system 2>/dev/null || \
        kubectl scale statefulset "$component" --replicas=0 -n nephoran-system 2>/dev/null || \
        echo "Component $component not found"
        
        # Measure detection time
        start_time=$(date +%s)
        echo "Waiting for failure detection..."
        sleep 30 # Simulate monitoring detection delay
        detection_time=$(($(date +%s) - start_time))
        
        # Execute recovery
        recovery_start=$(date +%s)
        ./component-failover.sh "$component" "restart"
        recovery_time=$(($(date +%s) - recovery_start))
        
        # Validate recovery
        ./post-recovery-validation.sh >/dev/null 2>&1
        validation_result=$?
        
        if [ $validation_result -eq 0 ]; then
          echo "‚úÖ $component: Recovery successful (${recovery_time}s)" | tee -a "$TEST_DIR/test-report.md"
        else
          echo "‚ùå $component: Recovery failed" | tee -a "$TEST_DIR/test-report.md"
        fi
      else
        echo "üß™ $component: Dry run - would test component failure" | tee -a "$TEST_DIR/test-report.md"
      fi
    done
    ;;
    
  "regional")
    echo "=== REGIONAL FAILOVER TESTING ==="
    echo -e "\n## Regional Failover Test" >> "$TEST_DIR/test-report.md"
    
    if [ "$TEST_ENV" != "staging" ]; then
      echo "‚ùå Regional testing only supported in staging environment"
      exit 1
    fi
    
    if [ "$DRY_RUN" = "false" ]; then
      # Test regional failover
      PRIMARY_REGION="us-central1-staging"
      FAILOVER_REGION="us-east1-staging"
      
      echo "Testing failover from $PRIMARY_REGION to $FAILOVER_REGION"
      
      # Execute planned failover
      failover_start=$(date +%s)
      ./regional-failover.sh "$PRIMARY_REGION" "$FAILOVER_REGION" "planned" > "$TEST_DIR/failover-log.txt" 2>&1
      failover_result=$?
      failover_time=$(($(date +%s) - failover_start))
      
      if [ $failover_result -eq 0 ]; then
        echo "‚úÖ Regional Failover: Successful (${failover_time}s)" | tee -a "$TEST_DIR/test-report.md"
        
        # Test failback
        echo "Testing failback to primary region..."
        failback_start=$(date +%s)
        ./regional-failover.sh "$FAILOVER_REGION" "$PRIMARY_REGION" "planned" > "$TEST_DIR/failback-log.txt" 2>&1
        failback_result=$?
        failback_time=$(($(date +%s) - failback_start))
        
        if [ $failback_result -eq 0 ]; then
          echo "‚úÖ Regional Failback: Successful (${failback_time}s)" | tee -a "$TEST_DIR/test-report.md"
        else
          echo "‚ùå Regional Failback: Failed" | tee -a "$TEST_DIR/test-report.md"
        fi
      else
        echo "‚ùå Regional Failover: Failed" | tee -a "$TEST_DIR/test-report.md"
      fi
    else
      echo "üß™ Regional: Dry run - would test regional failover" | tee -a "$TEST_DIR/test-report.md"
    fi
    ;;
    
  "full")
    echo "=== FULL DISASTER RECOVERY TESTING ==="
    echo -e "\n## Full Disaster Recovery Test" >> "$TEST_DIR/test-report.md"
    
    if [ "$TEST_ENV" != "staging" ]; then
      echo "‚ùå Full DR testing only supported in staging environment"
      exit 1
    fi
    
    if [ "$DRY_RUN" = "false" ]; then
      # Create pre-disaster baseline
      echo "Creating pre-disaster baseline..."
      kubectl get networkintents -A -o yaml > "$TEST_DIR/pre-disaster-intents.yaml"
      kubectl get pods -n nephoran-system -o wide > "$TEST_DIR/pre-disaster-pods.txt"
      
      # Simulate complete disaster
      echo "Simulating complete system failure..."
      disaster_start=$(date +%s)
      kubectl delete namespace nephoran-system --wait=true
      
      # Execute full system restore
      restore_start=$(date +%s)
      ./full-system-restore.sh "$(date +%Y%m%d)" "staging" > "$TEST_DIR/restore-log.txt" 2>&1
      restore_result=$?
      restore_time=$(($(date +%s) - restore_start))
      
      if [ $restore_result -eq 0 ]; then
        echo "‚úÖ Full System Restore: Successful (${restore_time}s)" | tee -a "$TEST_DIR/test-report.md"
        
        # Validate data consistency
        echo "Validating data consistency..."
        kubectl get networkintents -A -o yaml > "$TEST_DIR/post-restore-intents.yaml"
        
        if diff "$TEST_DIR/pre-disaster-intents.yaml" "$TEST_DIR/post-restore-intents.yaml" >/dev/null 2>&1; then
          echo "‚úÖ Data Consistency: Perfect match" | tee -a "$TEST_DIR/test-report.md"
        else
          echo "‚ö†Ô∏è  Data Consistency: Some differences found" | tee -a "$TEST_DIR/test-report.md"
          diff "$TEST_DIR/pre-disaster-intents.yaml" "$TEST_DIR/post-restore-intents.yaml" > "$TEST_DIR/data-diff.txt"
        fi
      else
        echo "‚ùå Full System Restore: Failed" | tee -a "$TEST_DIR/test-report.md"
      fi
    else
      echo "üß™ Full DR: Dry run - would test complete disaster recovery" | tee -a "$TEST_DIR/test-report.md"
    fi
    ;;
    
  *)
    echo "‚ùå Unknown test type: $TEST_TYPE"
    echo "Supported types: component, regional, full"
    exit 1
    ;;
esac

# Generate final test report
echo -e "\n=== DR TESTING COMPLETED ==="
echo -e "\n## Test Summary" >> "$TEST_DIR/test-report.md"

# Count results
passes=$(grep -c "‚úÖ" "$TEST_DIR/test-report.md" || echo "0")
failures=$(grep -c "‚ùå" "$TEST_DIR/test-report.md" || echo "0")
warnings=$(grep -c "‚ö†Ô∏è" "$TEST_DIR/test-report.md" || echo "0")

cat >> "$TEST_DIR/test-report.md" << SUMMARY

**Results Summary:**
- Passed: $passes
- Failed: $failures  
- Warnings: $warnings
- Overall Status: $(if [ "$failures" -eq 0 ]; then echo "‚úÖ PASSED"; else echo "‚ùå FAILED"; fi)

**Performance Metrics:**
$(if [ "$TEST_TYPE" = "component" ]; then echo "- Component Recovery Time: Average {CALCULATE}s"; fi)
$(if [ "$TEST_TYPE" = "regional" ]; then echo "- Regional Failover Time: ${failover_time:-N/A}s"; fi)
$(if [ "$TEST_TYPE" = "full" ]; then echo "- Full Restore Time: ${restore_time:-N/A}s"; fi)

**Test Duration:** $(($(date +%s) - $(date -d "$(grep 'Started:' <<< "$0")" +%s)))s
**Completed:** $(date -u)

## Recommendations
$(if [ "$failures" -gt 0 ]; then echo "- Address failed tests before production deployment"; fi)
$(if [ "$warnings" -gt 0 ]; then echo "- Review warnings and consider improvements"; fi)
- Schedule next DR test in 3 months
- Update procedures based on lessons learned

## Next Steps
- [ ] Review test results with team
- [ ] Update DR procedures if needed
- [ ] Schedule follow-up tests
- [ ] Update RTO/RPO targets if needed
SUMMARY

# Update report status
sed -i 's/Status: IN PROGRESS/Status: COMPLETED/' "$TEST_DIR/test-report.md"

if [ "$failures" -eq 0 ]; then
  echo "‚úÖ DR Testing PASSED ($passes tests)"
else
  echo "‚ùå DR Testing FAILED ($failures failures, $passes passes)"
fi

echo "Test report saved: $TEST_DIR/test-report.md"
echo "Test artifacts saved in: $TEST_DIR/"
EOF

chmod +x dr-testing-framework.sh
```

---

**Document Version**: 1.0  
**Last Updated**: 2025-01-07  
**Next Review**: 2025-02-07  
**Owner**: Nephoran Disaster Recovery Team  
**Approvers**: Operations Manager, Engineering Manager, Business Continuity Manager